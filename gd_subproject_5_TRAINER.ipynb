{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Training Script - Subproject5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from playsound import playsound\n",
    "\n",
    "from misc.utils import MyUtils\n",
    "from code_linear_regression.linear_regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training config\n",
    "max_degree = 3\n",
    "training_epochs = 10_000\n",
    "eta_list = [0.01, 0.001, 0.0001]\n",
    "lam_list = [5, 1, 0.1] \n",
    "\n",
    "# dataset config\n",
    "normalize_neg1_pos1 = False\n",
    "normalize_zero_one = True\n",
    "num_samples = None # set to None for all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./dataset/houseprice\"\n",
    "\n",
    "X_train = pd.read_csv(os.path.join(data_path, \"x_train.csv\")).to_numpy()[:num_samples]\n",
    "y_train = pd.read_csv(os.path.join(data_path, \"y_train.csv\")).to_numpy()[:num_samples]\n",
    "X_test = pd.read_csv(os.path.join(data_path, \"x_test.csv\")).to_numpy()\n",
    "y_test = pd.read_csv(os.path.join(data_path, \"y_test.csv\")).to_numpy()\n",
    "\n",
    "if normalize_neg1_pos1:\n",
    "    X_train = MyUtils.normalize_neg1_pos1(X_train)\n",
    "    y_train = MyUtils.normalize_neg1_pos1(y_train)\n",
    "    X_test = MyUtils.normalize_neg1_pos1(X_test)\n",
    "    y_test = MyUtils.normalize_neg1_pos1(y_test)\n",
    "    \n",
    "elif normalize_zero_one:\n",
    "    X_train = MyUtils.normalize_0_1(X_train)\n",
    "    y_train = MyUtils.normalize_0_1(y_train)\n",
    "    X_test = MyUtils.normalize_0_1(X_test)\n",
    "    y_test = MyUtils.normalize_0_1(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Metric Calculations\n",
    "For our training configurations in the above cells (`max_degree`, `training_epochs`, `eta_list`, and `lam_list`), we will train our model using gradient descent for all combinations of hyperparameters. The training results will be compiled into a dictionary and stored locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "results = [] # results will hold dict of (degree, epochs, eta, lam, train_mse, test_mse, y_hat)\n",
    "\n",
    "for r in range(1, max_degree + 1):  # 1-based indexing\n",
    "    print(f\"degree {r}\")\n",
    "    \n",
    "    print(f\"\\tepochs {training_epochs}\")\n",
    "\n",
    "    for eta_val in eta_list:\n",
    "        print(f\"\\t\\teta {eta_val}\")\n",
    "\n",
    "        for lam_val in lam_list:\n",
    "            print(f\"\\t\\t\\tlam {lam_val}\")\n",
    "\n",
    "            start = time.time()\n",
    "            train_mse, test_mse = lr.fit_metrics(X=X_train, y=y_train, X_test=X_test, y_test=y_test, epochs=training_epochs, eta=eta_val, degree=r, lam=lam_val)\n",
    "            end = time.time()\n",
    "\n",
    "            y_hat = lr.predict(X=X_test)\n",
    "\n",
    "            result = {\n",
    "                \"degree\": r,\n",
    "                \"epochs\": training_epochs,\n",
    "                \"eta\": eta_val,\n",
    "                \"lam\": lam_val,\n",
    "                \"train_mse\": train_mse,\n",
    "                \"test_mse\": test_mse,\n",
    "                \"min_train_mse\": min(train_mse),\n",
    "                \"min_test_mse\": min(test_mse),\n",
    "                \"y_hat\": list(y_hat.flatten()), # json doesnt like the nd-array\n",
    "                \"train_time\": (end - start) # trainng time in seconds\n",
    "            } \n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "            print(f\"\\t\\t\\tlam {lam_val} done;\")\n",
    "\n",
    "        print(f\"\\t\\teta {eta_val} done;\")\n",
    "\n",
    "    print(f\"\\tepochs {training_epochs} done;\")\n",
    "        \n",
    "    print(f\"degree {r} done;\")\n",
    "        \n",
    "assert len(results) == max_degree * len(eta_list) * len(lam_list)\n",
    "print(f\"\\nnumber of training runs: {len(results)}\")\n",
    "\n",
    "# notification when done\n",
    "playsound('./misc/change_da_world.mp3')\n",
    "\n",
    "# store output\n",
    "join_str = \"-\"\n",
    "gd_output_filename = f\"./output/{datetime.datetime.now()}_GD_degree-{max_degree}_epochs-{training_epochs}_eta-{join_str.join([str(int) for int in eta_list])}_lam-{join_str.join([str(int) for int in lam_list])}.json\".replace(\":\", \"-\").replace(\" \", \"_\")\n",
    "with open(gd_output_filename, \"w\") as file:\n",
    "    json.dump(results, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
